{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lihaocheng_i/.conda/envs/py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/lihaocheng_i/.conda/envs/py3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from model import Model\n",
    "from input_pipe import InputPipe\n",
    "from feeder import VarFeeder\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from trainer import predict\n",
    "from hparams import build_hparams\n",
    "import hparams\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(true, pred):\n",
    "    summ = np.abs(true) + np.abs(pred)\n",
    "    smape = np.where(summ == 0, 0, np.abs(true - pred) / summ)\n",
    "    return smape\n",
    "\n",
    "def mae(true, pred):\n",
    "    return np.abs(np.abs(true) - np.abs(pred))\n",
    "\n",
    "def mean_smape(true, pred):\n",
    "    raw_smape = smape(true, pred)\n",
    "    masked_smape = np.ma.array(raw_smape, mask=np.isnan(raw_smape))\n",
    "    return masked_smape.mean()\n",
    "\n",
    "def mean_mae(true, pred):\n",
    "    raw_mae = mae(true, pred)\n",
    "    masked_mae = np.ma.array(raw_mae, mask=np.isnan(raw_mae))\n",
    "    return masked_mae.mean()\n",
    "\n",
    "def predict_loss(prev, paths, split_df):\n",
    "    # prev: true value\n",
    "    # paths: paths to the model weights\n",
    "    t_preds = []\n",
    "    for tm in range(3):\n",
    "        tf.reset_default_graph()\n",
    "        t_preds.append(predict(paths, build_hparams(hparams.params_s32), back_offset=0, predict_window=288,\n",
    "                        n_models=3, target_model=tm, seed=2, batch_size=50, asgd=True, split_df=split_df))\n",
    "    preds=sum(t_preds) /3\n",
    "    preds.index = [idx.decode('ascii') for idx in preds.index]\n",
    "    # mean mae\n",
    "    res = 0\n",
    "    for idx in preds.index:\n",
    "        res += np.abs(preds.loc[idx, :] - prev.loc[idx, -288:]).sum()\n",
    "    res /= len(preds.index) * 288\n",
    "    return preds, res\n",
    "\n",
    "def split_data(df):\n",
    "    bad_path = os.path.join('/nfs/project/lihaocheng/badcase', 'single_rnn_mae_beyond_1000_vm_uuids')\n",
    "    bad_df = pd.DataFrame()\n",
    "    normal_df = df.copy()\n",
    "    with open(bad_path, 'r') as f:\n",
    "        line = f.readline()\n",
    "        while(line):\n",
    "            line = line[:-1] + \".hdf5\"\n",
    "            if line in df.index:\n",
    "                bad_df = bad_df.append(df.loc[line])\n",
    "                normal_df = normal_df.drop(line)\n",
    "            line = f.readline()\n",
    "    return bad_df.sort_index(), normal_df.sort_index()\n",
    "\n",
    "def show_single(preds, prev, vm, scope=288, bad_case=True):\n",
    "    name = preds.index[vm]\n",
    "    if bad_case:\n",
    "        bad_path = os.path.join('/nfs/project/lihaocheng/badcase', 'single_rnn_mae_beyond_1000_vm_uuids')\n",
    "        bad_list = []\n",
    "        with open(bad_path, 'r') as f:\n",
    "            line = f.readline()\n",
    "            while(line):\n",
    "                line = line[:-1] + \".hdf5\"\n",
    "                if line in preds.index:\n",
    "                    bad_list.append(line)\n",
    "                line = f.readline()\n",
    "        name = bad_list[vm]\n",
    "    \n",
    "    # mean mae for each row\n",
    "    print(f'vm name: {name}')\n",
    "    prev.loc[name, ends[vm] - scope : ends[vm]].plot(logy=True)\n",
    "    (preds.loc[name, :]).plot(logy=True)\n",
    "    # mean loss\n",
    "    print(mean_mae(prev.loc[name, ends[vm] - 288 : ends[vm]], preds.loc[name, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lihaocheng_i/.conda/envs/py3/lib/python3.6/site-packages/tables/leaf.py:414: PerformanceWarning: The Leaf ``/df/block0_values`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  PerformanceWarning)\n",
      "/home/lihaocheng_i/.conda/envs/py3/lib/python3.6/site-packages/pandas/io/pytables.py:734: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  columns=columns)\n"
     ]
    }
   ],
   "source": [
    "from make_features import read_all, read_pickle, find_start_end\n",
    "df_all = read_all()\n",
    "starts, ends = find_start_end(df_all.values)\n",
    "prev = df_all.apply(lambda x : np.exp(x) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abnormal vm model weight path : [['data/normal_cpt/s32/cpt-395']]\n",
      "INFO:tensorflow:Restoring parameters from data/bad_vars/feeder.cpt\n",
      "INFO:tensorflow:Restoring parameters from data/normal_cpt/s32/cpt-395\n",
      "0..............ðŸŽ‰\n",
      "INFO:tensorflow:Restoring parameters from data/bad_vars/feeder.cpt\n",
      "INFO:tensorflow:Restoring parameters from data/normal_cpt/s32/cpt-395\n",
      "0..............ðŸŽ‰\n",
      "INFO:tensorflow:Restoring parameters from data/bad_vars/feeder.cpt\n",
      "INFO:tensorflow:Restoring parameters from data/normal_cpt/s32/cpt-395\n",
      "0..............ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "bad_prev, normal_prev = split_data(prev)\n",
    "bad_paths = [p for p in tf.train.get_checkpoint_state(os.path.join('data/normal_cpt', 's32')).all_model_checkpoint_paths]\n",
    "print(f'Abnormal vm model weight path : [{bad_paths}]')\n",
    "bad_preds, bad_loss = predict_loss(bad_prev, bad_paths, 2)\n",
    "normal_paths = [p for p in tf.train.get_checkpoint_state(os.path.join('data/normal_cpt', 's32')).all_model_checkpoint_paths]\n",
    "print(f'Normal vm model weight path : [{normal_paths}]')\n",
    "normal_preds, normal_loss = predict_loss(normal_prev, normal_paths, 1)\n",
    "mae = (bad_loss * len(bad_preds.index) + normal_loss * len(normal_preds.index)) / (len(bad_preds.index) + len(normal_preds.index))\n",
    "print(f'Abnormal vm mean MAE = {bad_loss},\\nnormal vm mean MAE = {normal_loss},\\nall vm mean MAE = {mae}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'split_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-218cf1e93abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/cpt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_model_checkpoint_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Mean MAE = {loss}\\n........Generate csv for each csv..........'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-a20d8e5d719a>\u001b[0m in \u001b[0;36mpredict_loss\u001b[0;34m(prev, paths, split_df)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         t_preds.append(predict(paths, build_hparams(hparams.params_s32), back_offset=0, predict_window=288,\n\u001b[0;32m---> 26\u001b[0;31m                         n_models=3, target_model=tm, seed=2, batch_size=50, asgd=True, split_df=split_df))\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_preds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'split_df'"
     ]
    }
   ],
   "source": [
    "paths = [p for p in tf.train.get_checkpoint_state(os.path.join('data/cpt', 's32')).all_model_checkpoint_paths]\n",
    "preds, loss = predict_loss(prev, paths, split_df)\n",
    "print(f'Mean MAE = {loss}\\n........Generate csv for each csv..........')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_single(preds, prev, 110, 1288)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
